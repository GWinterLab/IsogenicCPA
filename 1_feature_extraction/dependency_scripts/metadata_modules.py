#############################################
# Author(s): Amanda Ng R.H.
# Created on: 10 Feb 2022
# Last updated on: 06 Apr 2023 (editing module 1 functions)
#
# Modules 1, 3, 6 and 8 (to be added) of the cpa_fature_extraction pipeline. These modules are for generating/updating the metadata file for CellProfiler pipelines to read and use in the cpa_feature pipeline or for organising the output of the cpa_fature_extraction.
#############################################

############
# Package(s)
############
# file management
import os
import shutil

# dataframe management
import pandas as pd

# array management
import numpy as np

# for interfacing with command line
import argparse

# for choosing numbers randomly
import random

# #######################
# # Dependencies tracking
# #######################
# freeze_cmd = "pip freeze > python_env_requirements.txt"
# os.system(freeze_cmd)
#
# print("The packages required for running the modules here have been saved in requirements.txt.")

################################################################################
# Dictionary containing the description and usage information on the function (see below)
################################################################################
function2info_dict = {
"make_metadata1_subset":
'''
Usage: python3 metadata_modules.py --make_metadata1_subset ${module_1_output} ${module_1subset_output}
Returns: A subset of the metadata sheet generated by module 1 to use in the test mode of the CPA feature extraction pipeline.
''',

"split_metadata1":
'''
Usage: python3 metadata_modules.py --split_metadata1 ${module_3_output} ${mini_metadata_dir} ${number_of_batches}
Returns: Multiple mini metadata files after splitting the original metadata file generated by module 3. The number of mini metadata files depends on the user-defined number_of_batches.
''',

"check_script_b_completion":
'''Usage: python3 metadata_modules.py --check_script_b_completion ${module_3_output} ${module_5b_output}
Returns: Pass/Fail depending on whether the number of segmentation masks predicted tallies with the number of sets of images being assessed.''',

"add_treatment_metadata":
'''
Usage: python3 metadata_modules.py --add_treatment_metadata ${transferlist} ${metadata_df}
Returns: A dataframe with the treatment metadata for each image set.
Adds the treatment information for each well to the metadata sheet containing the information on the images used for CellProfiler analysis.
''',

"make_metadata1":
'''
Usage: python3 metadata_modules.py --make_metadata1 ${plate_raw_data_dir} ${channel_annotation} ${plate_annotation} ${transferlist} ${module_1_output}
Returns: A metadata file for all the image sets for handling in CellProfiler for images from one plate.
Generates a metadata file containing the following for Cell Profiler's LoadData module:
- raw images by channel, where each row corresponds to 1 well.
- plate (which matches the DestinationPlate in the transferlist)
- run
- well
- site
- UpdatedImageNumber
- treatment (using the add_treatment_metadata function)
''',

"update_metadata1":
'''
Usage: python3 metadata_modules.py --update_metadata1 ${module_2_output} ${channel_annotation} ${module_3_output}
Returns: The metadata sheet generated by module 1 (i.e. by make_metadata1) updated with the paths and filenames of the illumination correction matrix/matrices.
Updates the metadata file generated by module 1 of the CPA feature extraction pipeline with the path and file names of the illumination correction matrix/matrices (output of module 2).
''',

"update_metadata2":
'''
Usage: python3 metadata_modules.py --update_metadata2 ${nuclei_segmentation_dir} ${whole_cell_segmentation_dir} ${module_3_output} ${module_6b_output}
Returns: The metadata sheet generated by module 3 (i.e. by update_metadata1) updated with the paths and filenames of the nuclei and whole cells segmentation masks generated by module 5b.
Updates the metadata file generated by module 3 of the CPA feature extraction pipeline with the path and file names of the segmentation masks generated by module 5b.
''',

"compile_object_csv":
'''
Usage: python3 metadata_modules.py --compile_object_csv ${module_7b_output} ${module_8_output}
Returns: Module 7b outputs one object CSV file per batch of images analyzed. This function merges these individual batch object CSV files into one object CSV file. It also updates the ImageNumber values as UpdatedImageNumber for Cells, Cytoplasm, Nuclei and Image objects. This UpdatedImageNumber allows the user to map the exact image file to the associated measurements in Cells, Cytoplasm, Nuclei and Image objects.
'''
### TO ADD INFORMATION ON MODUlE FOR HELP WHEN RUNNING THE FUNCTION VIA BASH
}

#############
# Function(s)
#############
### SUBSET MODULE: Making a subset of the metadata file that will be used when the pipeline is in test mode that takes one image set per treatment for the subset ###
# Note: I haven't used this function in a very long time now. It did work during the early testing phase of the feature extraction pipeline (i.e. around Feb 2022).
def make_metadata1_subset(module_1_output_compiled, module_1subset_output):
    """
    Function that takes a subset of metadata for each treatment. This function is ONLY used in test mode to
    test the feature extraction pipeline.
    
    :param module_1_output_compiled: Path to the compiled module 1 output. The path to any metadata sheet also works
        and you could use this module for testing specific parts of the feature extraction pipeline.
    :type module_1_output_compiled: str
    :param module_1subset_output: Path to export the subset to.
    :type module_1subset_output: str
    :return:
    """
    metadata_df = pd.read_csv(module_1_output_compiled)
    subset_type_list = ["DMSO", "Morpho", "Reported_MG"]

    # randomly choose one image set per treatment
    subset_df = pd.DataFrame()
    for type in subset_type_list:
        drug_list = metadata_df[metadata_df["Metadata_TreatmentType"] == type]["Metadata_Treatment"].tolist()
        drug_list = list(set(drug_list))
        for drug in drug_list:
            temp_df = metadata_df[metadata_df["Metadata_Treatment"] == drug]
            random_row = random.choice(list(temp_df.index))
            subset_df = subset_df.append(metadata_df.iloc[random_row])

    subset_df.to_csv(module_1subset_output, index = False)
    print("\nA subset of the metadata sheet has been generated for testing the pipeline.")
    return

### BATCHING MODULE: Splits a metadata file into mini metadata files so that subsequent scripts reliant on it analyse smaller batches of images ###
def split_metadata1(module_3_output, mini_metadata_dir, number_of_batches):
    """
    Function that splits a metadata file into mini metadata files, which can be fed into the feature extraction
    pipeline for parallel processing purposes.
    
    :param module_3_output: Path to the metadata file after module 2 (calculation of illumination correction matrix)
        and module 3 (appending the path to the illumination correction matrix to the metadata).
    :type module_3_output: str
    :param mini_metadata_dir: Path to the directory, where the mini metadata will be saved to as "{i}_mini.csv".
    :type mini_metadata_dir: str
    :param number_of_batches: Integer indicating the number of mini metadata files to split the original metadata
        file into.
    :type number_of_batches: int
    :return:
    """
    number_of_batches = int(number_of_batches) # ensure that the variable is an integer especially after pipping from bash to python
    metadata_df = pd.read_csv(module_3_output)
    i = 0
    for mini_metadata_df in np.array_split(metadata_df, number_of_batches):
        i += 1
        print("Length of mini metadata sheet %s" % str(i))
        mini_metadata_df.to_csv("%s/%s_mini.csv" % (mini_metadata_dir, i), index = False)
        print(len(mini_metadata_df))
    return

### CHECK MASTER SCRIPT COMPLETION MODULE ###
def check_script_b_completion(module_3_output, module_5b_output):
    """
    Function that checks the completion of part b of the feature extraction pipeline. This function is used
    in part c of the feature extraction pipeline. If "pass", part c will run. If "fail", part c will not
    continue running.
    
    :param module_3_output: Path to the metadata file output from module 3 (which appends the path to the 
        illumination correction matrix). This metadata file is used to calculate the expected number of
        segmentation masks from modlue 5 (nuclei and whole cell segmentation).
    :type module_3_output: str
    :param module_5b_output: Path to the parent directory with the segmentation masks (that are in sub-directories
        that correspond to each batch).
    :type module_5b_output: str
    :return:
    """
    dir2msg_dict = dict()
    metadata_df = pd.read_csv(module_3_output)
    expected_number_of_masks = len(metadata_df)
    for dir in os.listdir(module_5b_output):
        if dir == "nuclei_segmentation":
            actual_number_of_masks = len(os.listdir("%s/%s" % (module_5b_output, dir)))
            if actual_number_of_masks == expected_number_of_masks:
                msg = "Pass"
            else:
                missing = expected_number_of_masks - actual_number_of_masks
                msg = "Fail (%s)" % missing
            dir2msg_dict[dir] = msg
        elif dir == "whole_cell_segmentation":
            whole_cell_masks_parent_dir = "%s/%s" % (module_5b_output, dir)
            whole_cell_masks = os.listdir(whole_cell_masks_parent_dir)
            if "_cp_masks.png" in whole_cell_masks[0]:
                actual_number_of_masks = len(whole_cell_masks)
                if actual_number_of_masks == expected_number_of_masks:
                    msg = "Pass"
                else:
                    missing = expected_number_of_masks - actual_number_of_masks
                    msg = "Fail (%s)" % missing
                dir2msg_dict[dir] = msg
            else:
                for sub_dir in whole_cell_masks:
                    actual_number_of_masks = len(os.listdir("%s/%s/%s" % (module_5b_output, dir, sub_dir)))
                    if actual_number_of_masks == expected_number_of_masks:
                        msg = "Pass"
                    else:
                        msg = "Fail"
                    dir2msg_dict[sub_dir] = msg
    print("\nChecking if the number of predicted masks tallies with the metadata:\n")
    for dir in dir2msg_dict:
        print("\n%s\t\t%s" % (dir, dir2msg_dict[dir]))
    print("\n'Pass' indicates that the number of masks predicted tallies with the metadata while 'Fail' indicates that either the segmentation has not run to completion or there is an error in the segmentation/master script b/module 4b.")

    for msg in list(dir2msg_dict.values()):
        if msg.startswith("Fail"):
            msg = "Fail"
    else:
        msg = "Pass"
    print("\nOverall: %s\n" % msg)
    return

### MODULE 1: Making the initial metadata file. ###
def add_treatment_metadata(transferlist, metadata_df):
    # ERROR
    # TODO: Correct the mapping of treatments 'cause right now the mapping is done with only the well information, when it should be done with the destination plate AND well information
    # TODO: Test the edited version of this function
    """
    Function that adds the "Metadata_Treatment" to the initial metadata file.
    
    :param transferlist: Path to the CSV file with details on the treatment added to each well and plate.
    :type transferlist: str
    :param metadata_df: Dataframe that contains the information on the paths to the images and their corresponding
        channels.
    :type metadata_df: pd.DataFrame
    :return metadata_df: metadata_df with the "Metadata_Treatment" column.
    :rtype metadata_df: pd.DataFrame
    """

    # load the transferlist as a dataframe
    transferlist_df = pd.read_csv(transferlist)
    
    # for each plate
    plateMetadata_list = []
    for plate in set(transferlist_df["DestinationPlate"].tolist()):
        
        # Trim down the transferlist and the metadata to the data pertaining to the plate
        plateTransferlist = transferlist_df[transferlist_df["DestinationPlate"] == plate]
        plateMetadata = metadata_df[metadata_df["Metadata_Plate"] == plate]
        
        # Map the treatments to the wells in the metadata for the given plate
        well2drug_dict = dict()
        for i in plateTransferlist.index:
            well = plateTransferlist.loc[i, "DestWell"]
            drug = plateTransferlist.loc[i, "CompoundName"]
            well2drug_dict[well] = drug
        plateMetadata["Metadata_Treatment"] = plateMetadata["Metadata_Well"].map(well2drug_dict)
        
        # Append the plate metadata with the treatment information to the list of plate metadata
        plateMetadata_list.append(plateMetadata)
    
    # Concatenate the plate metadata into a single metadata dataframe
    metadata_df = pd.concat(plateMetadata_list)
    
    # Return the metadata with the treatment information added
    return(metadata_df)

def make_metadata1(raw_data_dir, channel_annotation, plate_annotation, transferlist, module_1_output):
    # TODO: Test the edited version of this function which should have the following changes:
    # 1. Metadata_Plate now matches the DestinationPlate in the transferlist.
    #    --> which should NOT pose a problem for CellProfiler's string limit
    #        (I have only tested seven-character Metadata_Plate values though)
    """
    Function that generates the initial metadata file. This metadata file contains information on the path to
    the images and which channels they correspond to. This file is needed for handling the images in CellProfiler.
    
    Note: Each plate of data will have a unique plate number (up to 99 plates). For more plates, you will have to
    change the renaming system.
    
    :param raw_data_dir: Path to the parent directory with the sub-directories containing raw images from the
        Opera Phenix. The sub-directories are by plate (default output from the Opera Phenix).
    :type raw_data_dir: str
    :param channel_annotation: Path to the CSV file containing the channel number and what channel it corresponds to.
        This file MUST be updated according to the experiment settings used on the Opera Phenix during imaging!
    :type channel_annotation: str
    :param plate_annotation: Path to the CSV file containing the DestinationPlate (in the transferlist) and its respective
        name of the folder of raw images in the raw_data_dir (e.g. "20230320_rko_wt_1__2023-03-20T08_16_51-Measurement 1").
        Typically, I export the images directly from the Opera Phenix to the raw_data_dir so the naming convention is
        <plate name I provided>__<date>T<time>-Measurement 1. The DestinationPlate will be used as the Metadata_Plate
        information that is subsequently used for mapping the Metadata_Treatments.
    :type plate_annotation: str
    :param transferlist: Path to the CSV file with details on the treatment added to each well and plate.
    :type transferlist: str
    :param module_1_output: Path to ethe directory where the metadata files will be exported to.
    :type module_1_output: str
    :return:
    """
    # define the metadata labels
    metadata_groups = ["Metadata_Plate", "Metadata_Run", "Metadata_Cell", "Metadata_Well", "Metadata_Site"]

    # create a reference dictionary for mapping the channel number to the channel name
    channel_dict = dict()

    # fill in the channel reference dictionary and add in the remaining metadata labels
    channel_df = pd.read_csv(channel_annotation)
    for channel_id, channel_name in zip(channel_df["channel_id"].tolist(), channel_df["channel_name"].tolist()):
        channel_dict[channel_id] = channel_name
        metadata_groups.append("Image_FileName_Orig%s" % channel_name)
        metadata_groups.append("Image_PathName_Orig%s" % channel_name)

    # check if the channel annotation dictionary was made correctly
    print("\nChannel annotation details extracted:")
    print("%s\t\t%s" % ("channel_id", "channel_name"))
    for channel_id in channel_dict:
        print("%s\t\t%s" % (channel_id, channel_dict[channel_id]))

    # make a reference dictionary for mapping the row numbers to letters
    row_dict = dict()
    for number, letter in list(enumerate(letter for letter in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ')):
        # for a 384-well plate which has 16 rows
        if number < 16:
            number += 1
            row_number = str(number).zfill(2)
            row_dict[row_number] = letter
            
    # create a reference dictionary for mapping the name of the folder containing the
    # raw images for a given plate imaged on the Opera Phenix
    # to the equivalent DestinationPlate in the transferlist
    plate_dict = dict()
    
    # fill in the plate reference dictionary
    plate_df = pd.read_csv(plate_annotation)
    for plate_raw_data_dir, plate in zip(plate_df["plate_raw_data_dir"].tolist(), plate_df["DestinationPlate"].tolist()):
        plate_dict[plate_raw_data_dir] = plate
    
    # check if the plate annotation dictionary was made correctly
    print("\nPlate annotation details extracted:")
    print("%s\t\t%s" % ("plate_raw_data_dir", "DestinationPlate"))
    for plate_raw_data_dir, plate in plate_dict.items():
        print("%s\t\t%s" % (plate_raw_data_dir, plate))

    #################################
    # Fill in the metadata dictionary
    #################################
    # for each plate
    for i, plate_raw_data_dir in enumerate(os.listdir(raw_data_dir)):
        print("\nplate %s: %s" % (i, plate_raw_data_dir))
        
        # make a dictionary for storing information on each of the metadata groups
        metadata_dict = dict()
        for metadata_group in metadata_groups:
            metadata_dict[metadata_group] = []

        # get the folder name from the plate_raw_data_dir
        folder_name = plate_raw_data_dir
        plate_raw_data_dir = "%s/%s" % (raw_data_dir, plate_raw_data_dir)

        # get the plate, cell line and run information
        cell, run = folder_name.split("__")
        plate = plate_dict[folder_name]

        # get the image path to the raw data
        image_path = r"%s/Images" % plate_raw_data_dir

        # get the list of files in the image_path in alphabetical order to ensure that the images corresponding to the same well end up in the same row
        file_list = os.listdir(image_path)
        image_list = [i for i in file_list if i.endswith(".tiff")]
        image_list.sort()

        # calculate the images per channel/expected number of rows in the metadata
        images_per_channel = int(len(image_list)/len(channel_df))

        # fill in the plate and run of each well
        metadata_dict["Metadata_Plate"] += [plate] * images_per_channel
        metadata_dict["Metadata_Run"] += [run] * images_per_channel
        metadata_dict["Metadata_Cell"] += [cell] * images_per_channel

        # segregate the paths to the images by the channel they're associated with based on the channel number that is indicated in the image's filename
        for image in image_list:
            channel_name = channel_dict["ch%s" % image[15]]
            metadata_dict["Image_FileName_Orig%s" % channel_name].append(image)
            metadata_dict["Image_PathName_Orig%s" % channel_name].append(image_path)

        # add in the well and site metadata using the images from one channel as a representative of all other channels
        rep_channel = channel_df["channel_name"].tolist()[0]
        for image in metadata_dict["Image_FileName_Orig%s" % rep_channel]:

            # extract the well the image corresponds to
            row_number = image[1] + image[2]
            row_letter = row_dict[row_number]
            col_number = image[4] + image[5]
            well = row_letter + col_number
            metadata_dict["Metadata_Well"].append(well)

            # extract the site (aka field) the image corresponds to
            site = image[7] + image[8]
            metadata_dict["Metadata_Site"].append(site)

        try:
            # convert the metadata dictionary into a dataframe
            metadata_df = pd.DataFrame.from_dict(metadata_dict)

            # add in the treatment metadata
            metadata_df = add_treatment_metadata(transferlist, metadata_df)

            # export the dataframe
            # Note: I had to rename the metadata files to something shorter
            # because the full path to the metadata file was getting too long and
            # cellprofiler seems to have a problem handling such long paths.
            metadata_location = "%s/plate_%s.csv" % (module_1_output, str(i + 1).zfill(2))
            metadata_df.to_csv(metadata_location, index = False)

            print("The metadata has been generated.\nRaw images: %s\nMetadata location: %s"
            % (plate_raw_data_dir, module_1_output))

        except ValueError:
            # check that the lengths of the list of values associated with each key is the same
            print("\nChecking the lengths of the list of values associated with each key i.e. metadata category:")
            print("%s\t\t%s" % ("metadata_group", "length of values associated"))
            for metadata_group in metadata_dict:
                print("%s\t\t%s" % (metadata_group, len(metadata_dict[metadata_group])))

            print("\nArray length mismatch. Metadata dataframe cannot be made. This error can arise when the number of images per channel is unequal (see the above lengths output). Please check if all the images from Opera has been transferred to the input folder.")

    # compile the metadata sheets for each plate together into a single metadata sheet
    compiled_filename = "module_1_output_compiled.csv"
    compiled_filename_path = "%s/%s" % (module_1_output, compiled_filename)
    module_1_output_list = os.listdir(module_1_output)
    if len(module_1_output_list) > 0:
        i = 0
        module_1_outputs = []
        for output in module_1_output_list:
            if output != compiled_filename:
                i += 1
                print("file %s: %s" % (i, output))
                output_path = "%s/%s" % (module_1_output, output)
                module_1_outputs.append(pd.read_csv(output_path))
        module_1_output_compiled = pd.concat(module_1_outputs)
        module_1_output_compiled.to_csv(compiled_filename_path, index = False)
    print("\nThe metadata generated by module 1 for each plate has been compiled.")

    return

### MODULE 3: Updating the metadata file made by module 1 with the paths to the illumination correction matrices generated by module 2. ###
def update_metadata1(module_2_output, channel_annotation, module_1_output_compiled, module_3_output):
    """
    Function that updates the metadata file by module 1 by appending the paths to the illumination correction
    matrices calculated by module 2.
    
    :param module_2_output: Path to the directory containing the illumination correction matrices calculated
        for each channel per plate.
    :type module_2_output: str
    :param channel_annotation: Path to theCSV file containing the channel number and what channel it corresponds to.
        This file MUST be updated according to the experiment settings used on the Opera Phenix during imaging!
    :type channel_annotation: str
    :param module_1_output_compiled: Path to the compiled metadata file by module 1.
    :type module_1_output_compiled: str
    :param module_3_output: Path to save the updated metadata file with the illumination correction matrices to.
    :type module_3_output: str
    :return:
    """
    # read the metadata sheet as a dataframe
    metadata_df = pd.read_csv(module_1_output_compiled)

    # get the number of images that require each illumination matrix
    images_per_channel = len(metadata_df)

    # get the channel names
    channel_df = pd.read_csv(channel_annotation)
    channel_name_list = channel_df["channel_name"].tolist()

    # fill in the appropriate path and file name(s) of the illumination correction matrix relevant to the channel
    for channel_name in channel_name_list:
        metadata_df["FileName_Illum%s" % channel_name] = metadata_df["Metadata_Plate"].astype(str) + "_Illum%s.npy" % channel_name
        metadata_df["PathName_Illum%s" % channel_name] = [module_2_output] * images_per_channel

    # export the dataframe
    metadata_df.to_csv(module_3_output, index = False)

    print("\nThe metadata sheet on the images has been updated with that of the illumination correction matrix/matrices.\nIllumination correction matrix/matrices location: %s\nUpdated metadata location: %s"
    % (module_2_output, module_3_output))

    return

### MODULE 6b: Updating the metadata file made by module 3 with the paths to the nuclei and whole cell segmentation masks generated by CellPose in module 5b. ###
def update_metadata2(nuclei_segmentation_dir, whole_cell_segmentation_dir, module_3_output, module_6b_output):
    """
    Function that updates the metadata file by module 3 with the paths to the segmentation masks.
    
    :param nuclei_segmentation_dir: Path to the directory with the nuclei segmentation masks in PNG format.
    :type nuclei_segmentation_dir: str
    :param whole_cell_segmentation_dir: Path to the directory with the whole cell segmentation masks in PNG format.
    :type whole_cell_segmentation_dir: str
    :param module_3_output: Path to the metadata file by module 3.
    :type module_3_output: str
    :param module_6_output: Path to export the updated metadata file to.
    :type module_6_output: str
    :return:
    """
    # read the metadata sheet as a dataframe
    metadata_df = pd.read_csv(module_3_output)

    # original number of columns in the metadata_df
    orig_cols = metadata_df.shape[1]

    # get the number of image sets that require the segmentation mask
    images_per_channel = len(metadata_df)

    # reference dictionary for mapping the folder path to the nickname CellProfiler will be using to refer to the image/file
    folderpath2nickname_dict = dict()
    folderpath2nickname_dict[nuclei_segmentation_dir] = "NucleiMask"
    folderpath2nickname_dict[whole_cell_segmentation_dir] = "CellMask"

    # updating the metadata_df with the information on the masks
    for folderpath in folderpath2nickname_dict:
        try:
            nickname = folderpath2nickname_dict[folderpath]

            metadata_df["Image_PathName_%s" % nickname] = [folderpath] * images_per_channel

            example_file = os.listdir(folderpath)[0]
            example_file_suffix = example_file.split("__")[1]

            filename_list = []
            for row in metadata_df.index:
                plate = metadata_df.loc[row, "Metadata_Plate"]
                well = metadata_df.loc[row, "Metadata_Well"]
                site = metadata_df.loc[row, "Metadata_Site"]
                filename = "%s_%s_s%s__%s" % (plate, well, site, example_file_suffix)
                filename_list.append(filename)

            metadata_df["Image_FileName_%s" % nickname] = filename_list

        except:
            print("Oh no! Something went wrong with updating the metadata sheet with the segmentation masks. This error may happen because: (1) the following folder is not in folderpath2nickname_dict and will not be included in the metadata sheet. If you would like to include it in the metadata sheet, please update the folderpath2nickname_dict;\n(2) or the folder is empty.\n\t%s\n" % folderpath)

    # export the updated metadata sheet
    if metadata_df.shape[1] > orig_cols:
        metadata_df.to_csv(module_6b_output, index = False)
        print("\nMetadata sheet on the images has been updated with that of the CellPose segmentation masks.")
    else:
        print("\nERROR with updating the metadata file.")

    return

### MODULE 8: Compile the CSV files and updated the ImageNumber for Cells.csv and Cytoplasm.csv ###
# !! DO NOT USE !!
# The functions for this module do work, but I find that merging individual batch object CSV files
# at this point consumes too much RAM unnecessarily. Merger can be done during profile assembly,
# where the feature measurements are aggregated by site. The aggregation reduces the RAM usage by
# 100-fold without impinging on the data analysis.
def _compile_object_csv(input_dir, output_dir):
    """
    Internal function for compiling the individual CSV files from module 7b into single CSV files per object/
    Image/Experiment.
    Note: This function is NO LONGER used in the feature extraction pipeline due to excessive RAM consumption
    during profile assembly downstream.
    
    :param input_dir: Path to the directory with the individual CSV files from module 7b (one for each batch).
    :type input_dir: str
    :param output_dir: Path to the directory to export the merged CSV files.
    :type output_dir: str
    :return:
    """
    # list of expected objects with measurements saved as the CSV files
    obj_list = ["Cells", "Cytoplasm", "Nuclei", "Experiment", "Image"]

    ### map the paths to the various CSV files to each object type ###
    # dictionary for mapping the paths to the object CSV files to the object type
    obj2pathlist_dict = dict()
    for obj in obj_list:
        obj2pathlist_dict[obj] = []

    # loop through the output from each batch of images from which the features have been extracted
    # it won't follow the numerical order of the batch number but that doesn't matter
    for batch_output in os.listdir(input_dir):
        batch_output_path = r"%s/%s" % (input_dir, batch_output)

        # and save the path to the files by object to the above dictionary
        if os.path.isdir(batch_output_path):
            for file in os.listdir(batch_output_path):
                if file.endswith(".csv"):
                    file_path = r"%s/%s" % (batch_output_path, file)
                    obj, ext = file.split(".", 2)
                    obj2pathlist_dict[obj].append(file_path)

    print("A dictionary for objects mapped to paths the CSV files has been created.")

    for obj in obj2pathlist_dict:

        ### update the ImageNumber values for Cells, Cytoplasm, Nuclei and Image ###
        if obj in ["Cells", "Cytoplasm","Nuclei", "Image"]:
            i = 0
            for path in obj2pathlist_dict[obj]:
                i += 1
                df = pd.read_csv(path)
                last_row_number = len(df) - 1

                # for the first CSV file
                # and get the maximum ImageNumber value
                if i == 1:
                    df["UpdatedImageNumber"] = df["ImageNumber"]
                    max_imagenumber = df.loc[last_row_number, "UpdatedImageNumber"]

                # for the subsequent CSV files
                # add the UpdatedImageNumber which is ImageNumber + max_imagenumber
                # update the max_imagenumber which is the maximum value in the UpdatedImageNumber column
                else:
                    df["UpdatedImageNumber"] = df["ImageNumber"] + max_imagenumber
                    max_imagenumber = df.loc[last_row_number, "UpdatedImageNumber"]

                # export the dataframes with the UpdatedImageNumber added
                df.to_csv(path, index = False)
            print("%s --- UpdatedImageNumber added" % obj)

        ### merge the CSV files by objects they correspond to ###
        df = pd.concat(map(pd.read_csv, obj2pathlist_dict[obj]), ignore_index = True)
        merged_csv_filename = r"%s/%s_merged.csv" % (output_dir, obj)
        df.to_csv(merged_csv_filename, index = False)
        print("%s --- merged CSV files" % obj)

    print("")

    print("The CSV files have been merged by the object they correspond to.")
    return


def compile_object_csv(module_7b_output, module_8_output):
    """
    Function for compiling the individual object CSV files from module 7b (one for each batch). It can deal with
    the use of multiple whole cell segmentation models or a single whole cell segmentation model.
    Note: This function is NO LONGER used in the feature extraction pipeline due to excessive RAM consumption
    during profile assembly downstream.
    
    :param module_7b_output: Path to the directory with the individual object CSV files, which can be
        grouped into multiple sub-drectories by whole cell segmentation model used OR there a single whole
        cell segmentation model is used. In the latter case, the sub-directories will correspond directly to the
        batches.
    :type module_7b_output: str
    :param module_8_output: Path to the directory to export the merged CSV files to.
    :type module_8_output: str
    :return:
    """
    try:
        int(os.listdir(module_7b_output)[0])
        print("DETECTED: Only one whole cell segmentation model was used.")
        _compile_object_csv(module_7b_output, module_8_output)
    except ValueError:
        print("DETECTED: More than one whole cell segmentation model was used.")
        for folder in os.listdir(module_7b_output):
            input_dir = r"%s/%s" % (module_7b_output, folder)
            print("Folder in use: %s\n" % folder)

            # make the sub-directory for saving the merged CSV files to by whole cell segmentation model
            output_dir = r"%s/%s" % (module_8_output, folder)
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)

            _compile_object_csv(input_dir, output_dir)

    return

###########################################################
# Set up interfacing of function(s) for use in command line
###########################################################
if __name__ == "__main__":

    # if the user types --help
    parser = argparse.ArgumentParser(description = "Functions in metadata_modules.py: metadata generation/update and output management for the CPA feature extraction pipeline.")

    # if the user types --help for a specific function
    parser.add_argument("--make_metadata1_subset", nargs = 2, help = function2info_dict["make_metadata1_subset"])
    parser.add_argument("--split_metadata1", nargs = 3, help = function2info_dict["split_metadata1"])
    parser.add_argument("--check_script_b_completion", nargs = 2, help = function2info_dict["check_script_b_completion"])
    parser.add_argument("--make_metadata1", nargs = 5, help = function2info_dict["make_metadata1"])
    parser.add_argument("--update_metadata1", nargs = 4, help = function2info_dict["update_metadata1"])
    parser.add_argument("--update_metadata2", nargs = 4, help = function2info_dict["update_metadata2"])
    parser.add_argument("--compile_object_csv", nargs = 2, help = function2info_dict["compile_object_csv"])
    ### TO ADD HELP FOR MODULE

    # get the arguments from the user's input in command line
    args = parser.parse_args()

    # map the user's input from command line to the appropriate function
    if args.make_metadata1_subset:
        module_1_output, module_1subset_output = args.make_metadata1_subset
        make_metadata1_subset(module_1_output, module_1subset_output)
    elif args.split_metadata1:
        module_3_output, mini_metadata_dir, number_of_batches = args.split_metadata1
        split_metadata1(module_3_output, mini_metadata_dir, number_of_batches)
    elif args.check_script_b_completion:
        module_3_output, module_5b_output = args.check_script_b_completion
        check_script_b_completion(module_3_output, module_5b_output)
    elif args.make_metadata1:
        raw_data_dir, channel_annotation, plate_annotation, transferlist, module_1_output = args.make_metadata1
        make_metadata1(raw_data_dir, channel_annotation, plate_annotation, transferlist, module_1_output)
    elif args.update_metadata1:
        module_2_output, channel_annotation, module_1_output_compiled, module_3_output = args.update_metadata1
        update_metadata1(module_2_output, channel_annotation, module_1_output_compiled, module_3_output)
    elif args.update_metadata2:
        nuclei_segmentation_dir, whole_cell_segmentation_dir, module_3_output, module_6b_output = args.update_metadata2
        update_metadata2(nuclei_segmentation_dir, whole_cell_segmentation_dir, module_3_output, module_6b_output)
        
    # !! DO NOT USE THIS FUNCTION !! #
    # See comments for module 8 above for more details
    elif args.compile_object_csv:
        module_7b_output, module_8_output = args.compile_object_csv
        compile_object_csv(module_7b_output, module_8_output)
    ### TO ADD MAPPING FOR BASH INPUT TO MODULE ###
